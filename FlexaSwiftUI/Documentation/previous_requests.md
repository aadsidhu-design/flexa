FlexaSwiftUI(9970,0x1ef92c380) malloc: xzm: failed to initialize deferred reclamation buffer [2] (null)
10.29.0 - [GoogleUtilities/AppDelegateSwizzler][I-SWZ001014] App Delegate does not conform to UIApplicationDelegate protocol.
üîê API keys stored securely in Keychain
üîê Security Status:
¬†¬† Gemini API Key: ‚úÖ Configured
  Firebase Config: ‚ùå Removed (use Appwrite environment keys APPWRITE_ENDPOINT/APPWRITE_API_KEY)
¬†¬† Appwrite Config: ‚úÖ Configured
Notification permission granted
Gemini recommendation generation failed, falling back to rule-based only: apiError
nw_connection_copy_connected_local_endpoint_block_invoke [C8] Client called nw_connection_copy_connected_local_endpoint on unconnected nw_connection
nw_connection_copy_connected_remote_endpoint_block_invoke [C8] Client called nw_connection_copy_connected_remote_endpoint on unconnected nw_connection
nw_connection_copy_protocol_metadata_internal_block_invoke [C8] Client called nw_connection_copy_protocol_metadata_internal on unconnected nw_connection
Gemini recommendation generation failed, falling back to rule-based only: apiError
App is being debugged, do not track this hang
Hang detected: 0.56s (debugger attached, not reporting)
Unable to open mach-O at path: /Library/Caches/com.apple.xbs/Binaries/RenderBox/install/Root/System/Library/PrivateFrameworks/RenderBox.framework/default.metallib¬† Error:2
App is being debugged, do not track this hang
Hang detected: 0.55s (debugger attached, not reporting)
A new orientation transaction token is being requested while a valid one already exists. reason=Fullscreen transition (dismissing): fromVC=<_TtGC7SwiftUI29PresentationHostingControllerVS_7AnyView_: 0x131b10700>; toVC=<_TtGC7SwiftUI29PresentationHostingControllerVS_7AnyView_: 0x131b10000>;; windowOrientation=portrait; sceneOrientation=portrait; existingTransaction=<_UIForcedOrientationTransactionToken: 0x13225c480; state: active; originalOrientation: portrait (1)> the app sucks. the ui sucks. the optimization sucks. the animations are so memory and intensive. the battery usage is high. the app basically does not even function tbh. hmoe screen, it doesnt look like apple fitness UI, and hte 3 circles i dont want them to be next to eahc othe rin a linei want some pyramid type of configurationa nd for it to look like apple fitness acitvity rings kind of. and then the othe rstuff ont he screen loks super sloppy. then there is the progresspage. what the hell is that. i just want ONE SET GRAPH, LET ME SHOW YOU AN EXAMPLE  look at the 3rd phoot the one htat looks like apple fitness ui that is apple fitness ui its one big graph and then i want you to ahve the same button underneaht the View Details underneath. and hten on top dont have day month year, instead have the titleof the graph on top left, and thne a select session drop down top right. and then the title and select session are both dropdowns select sessiong ives the user the ability to select any sesson they have done so nme the sessions with exercise name, dateand time. super simple. andhten the title of theg raph should be AI Score, but hwen the user clicks on it its a dropdown where ht euser can pick ROM (this is per rep) SPARC, Pain Levels, Accelerometer, and Gyrosocpe. ai score and pain levels will be a graph showing the difference in pain levels over the week. ai score will be just ai score over the week. the rest will be like for a speciifc session so the user has to pick a specific session to load the graph. and then please make all this super simple you like make this so fucking complicated and overcomplicate htings make like 9 services to fucking do this and then there will be like 19 errors please be simple and efficinet and optimized in your code optimmize it. and hten the games. what th efuck are th egames. first of all i already say start exercise on the FUCKIONG INSTRUCTONS SCREEN WHY DO I NEED TO SAY IT AGAIN ON THE GAME PAGE. AND THEN WHEN I ENTER LETS SAY FRUIT SLCIER FOR EXAMPLE WHY THE FUCK DO I SEE A CAMERA PREVIEW? HANDHELD GAMES DONT NEED CAMERA PREVIEWS AND THENHTERE IS JUST A FUCKING HORIZONTAL BLACK BACKGROUND IN THE MIDDLE AND I HAVE TO CLICK START GAME??? I ALREADY CLICKED START GAME ONT HE INSTRUCITONS PAGE WHY DO I NEED TO DO IT AGAIN. AND THEN THE FUCKING GAME DOESNT EVEN FUCING WORK? THE RED DOT IN MIDDLE OF SCREENIS NOT RESPNSIVE TO MY MOVEMENTS. DONT HAVE NO FUCKING "PENDULUM DETEVTON, MOVMEENT DETECTION" LIKE JUST FUCKING WORK. MAKE THE SLICER GO UPA ND DOWN BASED ON ME FORWARD AND BACKWARD ACCELERATION MOVOEMNET (REMMEBER OFFSEET GRAVITY BCI DONT ANT TITLT CONTORLLINGTH EBALL). AND HTNE FRUITS NEED TOF UCIGN SPAWN FROM BOTTOM FO SCREEN, BOMBS TOO, WHERE IS HTE BOMB COUNTER TOP LEFT, HMMMMMM. AND HTNE THE FUCKING RESULTS PAGE. WHO AMNY TIMES DO I NEED TO TELL YOU HOW IT FUCKING LOOKS LIKE???? IT SHOUDL LOOK LIKE AI SCORE AT TOP, UNDERNEAHT IS AI FEEDBACJ, UNDER THAT IS A GRAPH WHERE USERS CAN PICK TO SEE ROM OR SPARC AND HTNE A DONE AND RETRY BUTTON BELOW THAT. EASY. AMKE SURE ALL GAMES ARE SUPER OPTIMIZED. AND THEN DO U EVEN LIKE USE APPLE VISION? WHAT I WANT YOU TO DO IS GO THROUGH THE GAMES SCREEN AND BASICALLY REMOVE EVERYTHING AND REDO ALL THE GAMES SO THEY ARE PLAYABLE AND COMPLETE GAMES NOT SOME PREVIEW BULLSHIT, THEN GO THROUGH THE SERVICES AND REMOVE ALL THE ROM SENSOR MOVMENET SERVICES AND FUCKING REDO IT < AMKE IT SIMPLE AND ORGANIZED AND EFFICIENT ALGORITHMS AND FORMULAS WTV, RMEMEBER TO USE APPLE VIISON. THEN GO INTO COMPONENT VIEWS AND CLEAN IT UP RMEOV EHT EOLD STUFF OR THGINS WE DONT USE. AND THEN MAKE SURE GAMES OWKR SENSOR SERVICE STARTS RIGHT AWAY WHEN WE GET INTOT HE GAME, ON SUNTRCITONS PAGE ONWE WE CLICK START GAME IT STARTS RIGHT AWAY. AND THEN APPLE VISION STARTS RIGHT AWAY TOO. Global goals (must meet)
Frame rate: 60 FPS sustained for games and camera overlays. No missed frames > 16.7 ms on the render loop.
Input latency: motion ‚Üí on-screen response < 50 ms; camera pose ‚Üí overlay response < 120 ms.
Threading: Main thread only for UI. All sensor reading, filtering, SPARC, rep detection, and Vision runs on background queues.
No fake data: all stats (Reps, ROM, SPARC, score, bombs, brew progress, nails) derive from live sensor/Vision signals.
Energy: keep device temp under control; use adaptive frame sampling and pause processing when app not visible.Stack & architecture (do this)
SwiftUI app shell + SpriteKit per game scene for simple, fast 2D; AVCaptureVideoPreviewLayer for camera preview overlays.
CoreMotion for handheld games (Fruit Slicer, Hammer Time, Witch Brew).
Vision (VNDetectHumanBodyPoseRequest) for camera-based exercises (Balloon Pop, Arm Raises, Wall Climbers).
Combine publishers for sensor and game events. One GameEngine per game, one MetricsEngine shared.
Games (handheld ‚Äì phone in hand; CoreMotion)
Common motion pipeline (ALL handheld games)
Enable DeviceMotion with attitude, userAcceleration, rotationRate @ 100 Hz (or highest available).
Apply filter chain (background queue):
Gravity separation using CMDeviceMotion.gravity.
Low-pass for attitude (cutoff ~ 3 Hz).
Band-pass for periodic reps where needed (0.2‚Äì2.0 Hz).
Compute pendulum phase when needed.
Publish MotionState at 60 Hz aligned with display (CADisplayLink) using the latest sample (triple-buffer).
Latency target: < 50 ms sensor‚Üíscreen. No allocations per frame.
SPARC (Spectral Arc Length) computation (shared)
For each rep windowed segment (1‚Äì5 s), compute:
Interpolate motion trace to uniform grid (60 Hz),
Normalize amplitude,
FFT magnitude spectrum,
Spectral Arc Length = ‚Äì‚àë sqrt( (Œîf)^2 + (ŒîM)^2 ) across normalized band (0‚Äì10 Hz),
Higher (less negative) = smoother. Store per rep and average.
Run on background queue; don‚Äôt block render.
ROM (Range of Motion)
Handheld games still estimate joint ROM via Vision (front camera not used here) OR via angle proxy:
Preferred: If user permits, rear camera Vision in analysis segment (post-play) to measure ROM with arm extended. If not, estimate from device pitch/roll amplitude and map to degrees using per-user calibration.
For Fruit Slicer/Witch Brew: Shoulder (glenohumeral) abduction/flexion ROM.
For Hammer Time: Elbow flex/ext ROM.
Store ROM per rep.

Game 1: Fruit Slicer (inspired by Fruit Ninja)
Screen: Portrait. No camera preview (just the game).
‚ÄúSlicer‚Äù == red dot constrained only vertically in center x.
Mechanics
Map pendulum forward/back ‚Üí dot Y:
Forward swing (away) ‚Üí dot moves up.
Backward ‚Üí down.
Clamp X to screen midpoint; Y maps linearly from filtered device pitch (or pendulum velocity integrated) to screen height.
Use small smoothing (EMA Œ±‚âà0.2) to reduce jitter without lag.
Fruit spawns from bottom, random X near bottom, launch angle aimed toward center (dot) with slight noise. Parabolic SKAction (gravity).
Bombs spawn rarely (p‚âà0.05 of spawns). Identical arc; visually distinct.
Slice detection: If dot intersects fruit‚Äôs hitbox while dot velocity magnitude > threshold, mark fruit sliced.
Score: +1 per fruit. No timer. Top-center score label.
Bomb counter: 3 icons top-left. On hit: mark one with X. After 3, Game Over ‚Üí Analyzing.
Analyzing: compute SPARC, ROM, reps (count up-crossings of velocity/angle).
Acceptance:
Dot follows vertical movement synchronously, no visible lag.
Fruits arc toward center; consistent difficulty curve.
Bombs are clearly avoidable; 3 strikes ends game.
60 FPS on iPhone 12 or newer.

Game 2: Hammer Time
Screen: Portrait game, but motion mapping is landscape-style (user lying on side).
Visual: Center double-sided hammer; walls on left & right with visible nails; simple ‚Äúframed painting‚Äù props on the walls.
Mechanics
Map elbow/forearm up-down to hammer left-right:
Use roll or forearm vertical acceleration integrated ‚Üí position X.
Add small spring/damper to make hammer feel weighty.
Goal: Drive nails flush into walls. Each side has 1 nail (or 2 on higher difficulty).
On hammer hit (collider with nail head) with velocity>threshold ‚Üí increment nail depth.
When all nails flush ‚Üí Game Over ‚Üí Analyzing.
UI: Progress bars above each nail indicating depth to flush.
Acceptance:
Hammer tracks user motion with minimal lag.
Clear feedback on hits (haptic + spark particles).
Game ends exactly when all nails are flush; analysis runs.

Game 3: Witch Brew
Screen: Portrait, stylized cauldron centered.
Mechanics
Phone pendulum circular motion ‚Üí stirrer moving circularly inside cauldron.
Compute 2D phase from pitch/roll to drive stirrer around circle.
Ingredients drop periodically; stirring rate changes brew color and spawns bubbles/steam.
Brew progress bar (yellow-green) under cauldron fills with sustained stirring, decays quickly when idle.
When progress full ‚Üí show witch face above cauldron ‚Üí Analyzing.
Acceptance:
Stirrer path is smooth circular; breaks if user reverses direction.
Progress noticeably decays when user stops; completion reliably triggers.

Camera-based exercises (phone stationary; front camera preview visible)
Common camera pipeline
AVCaptureSession front camera 720p; AVCaptureVideoPreviewLayer fills screen behind overlay.
Run VNDetectHumanBodyPoseRequest on a background queue at ~20‚Äì30 FPS, throttled by device.
Track keypoints: wrists, elbows, shoulders, index/middle fingers (where available).
Overlays are SpriteKit nodes drawn above the preview.
Rep detection: angle time series with hysteresis thresholds.
ROM per rep: peak-to-peak joint angle per detected rep.
SPARC: computed across each rep‚Äôs angle trace (see common SPARC).
Acceptance: Preview is live with overlays; no stutters; Vision runs off the main thread.
Exercise A: Balloon Pop (Elbow extensions)
Balloons spawn above user (overlay space) drifting slightly.
A pin/knife overlay ‚Äústicks‚Äù to detected hand position (dominant hand; choose with highest confidence).
Pop when hand collider intersects balloon (and elbow extension angle increasing).
On pop: score++, spawn another balloon.
ROM metric: Elbow angle (upper arm‚Äìforearm) per pop rep.
End condition: time-boxed or N pops. Then Analyzing.
Exercise B: Arm Raises (front or lateral)
User chooses direction implicitly; detect shoulder angle w.r.t. torso.
Overlay targets (rings) appear and rise as user raises arm; hitting targets triggers chime.
Count reps on raise + controlled lower cycles.
Record ROM per rep (max shoulder angle).
Analyzing after N reps or time.
Exercise C: Wall Climbers (index/middle finger climb)
Detect plane/wall via simple vertical plane inference (coarse) or assume wall in front; more robust: detect finger keypoints path up the frame.
Mask the wall area with a mountain texture; simulate background moving down as user climbs.
Elevation bar at side shows ascent.
Reps counted as discrete hand ‚Äústeps‚Äù upward; ROM = shoulder angle span.
Metrics & detection specifics
Rep detection (generic)
Use joint angle time series.
Thresholds with hysteresis: e.g., extension threshold Œ∏_high, flexion threshold Œ∏_low.
Rep counted on crossings Œ∏_low‚ÜíŒ∏_high‚ÜíŒ∏_low with min duration (e.g., 0.6 s) and min amplitude (‚â• 10‚Äì15¬∞).
Debounce to avoid double-counts.
ROM (per rep)
ROM = max(angle) - min(angle) within rep window.
Store and display mean ¬± SD and per-rep list.
SPARC
Implement once; run on background queue; attach to each rep.
Show session average and per-rep sparkline. LOOK AT THE FIRST TWO IMAGES TO SEE HOW SHIT THE UI IS BY THE WAY FOR REFERENCE. AND LOOK ONLINE FOR UI FOR APPLE FINTESS. AND HTEN WHEN I SAY VN DETECTHUNANBODYPOSEREQUEST USE APLPEV ISIO PLEASE USE ITS PSOE DTECTION OKAY??? MAKE SURE EVERYTHING WORKS DOUBLE CHECK YOUR WORK, CREATE A PLAN, FOLLOW THE PLAN, TICK OFF THE LIST. DO THIS STEP BY STEP THOROUGHLY. Done definition (don‚Äôt mark complete unless)
All acceptance checks pass.
No placeholder values anywhere in UI.
Performance budgets met on iPhone 12+.
Latest session immediately reflected in Progress and Goals.Please be thorough in your implementation and double check every part of the codebase to ensure it is optimized, efficient, and meaningful. Avoid placeholders or shortcuts‚Äîmake sure every feature is hooked up properly and works exactly as described. Pay close attention to latency, frame rate, and responsiveness so the gameplay feels smooth and natural. Confirm that calculations like SPARC, ROM, and rep detection are correct and connected to live data, not mock values. Review your code for unnecessary allocations, redundant logic, or main-thread blocking tasks, and clean it up for performance. Get rid of any unused services, outdated pages, or redundant code left over from earlier versions so the project is lean and maintainable. Most importantly, validate that the user experience matches my detailed specifications and nothing is left incomplete.did ya do this? Scan Swift files for problematic [weak self] in SwiftUI views and direct calls to actor-isolated motionService methods
Fix [weak self] misuse in game views (BalloonPop, WallClimbers, ArmRaises, HammerTime, FruitSlicer) for Timer/closures
Wrap all motionService.start/stop calls in Task { @MainActor in ... }
Resolve minor warnings (unused variables -> _, explicit self, optionals)
Build and run to verify no compile errors and main screens show and then do this. I'll first plan the tasks and track progress. Then I'll open and review your motion and game view files to understand current sensor usage. I'll also search the codebase for any usage of Apple's Vision framework or Core Motion. In parallel, I'll run targeted web searches on how to use Apple's Vision framework for pose/hand detection and on Core Motion best practices to compare with your implementation.

I'll inspect your motion service and game views to understand current sensor usage, and search the codebase for Vision/Core Motion usage. In parallel, I'll start web research on Apple's Vision (pose/hand) and Core Motion best practices to compare with your implementation. Running these in parallel for speed.

I'll search the codebase for Vision and Core Motion usages and open the main motion service to validate implementation details against best practices. Running searches and reading the relevant files in parallel for speed.

I'll open a few more relevant files to confirm how Vision and Motion are used, and then research Apple's official guidance on Vision pose detection, AVCapture, and Core Motion best practices to validate against our code.

I'll open `Config/Info.plist` to verify required privacy usage descriptions (e.g., camera) are present, since Vision and AVCapture require them.

I'll update our TODO list progress and save a key architecture observation in memory so we don't lose it.

# Findings

- __Multiple camera sessions and inconsistent Vision hookup__
  - `OptimizedMotionService.setupCameraSession()` starts an `AVCaptureSession` and feeds `VNDetectHumanBodyPoseRequest` in `captureOutput(_:didOutput:)` ([OptimizedMotionService.swift](cci:7://file:///Users/aadi/Desktop/FlexaSwiftUI/FlexaSwiftUI/Services/OptimizedMotionService.swift:0:0-0:0)).
  - Game views overlay `CameraPreviewView()` (which starts its own session) and a `SpriteView`:
    - `OptimizedArmRaisesGameView` uses `CameraPreviewView()` but never calls `ArmRaisesCameraPreviewView(scene:)`, so `ArmRaisesScene.setupCameraPreview(in:)` is never invoked. Its `AVCaptureVideoDataOutput` delegate will not fire.
    - `OptimizedBalloonPopGameView` uses `BalloonCameraPreviewView` with `CameraPreview()`; `BalloonPopScene` never sets up its own session but defines a sample-buffer delegate ‚Äî also won‚Äôt fire.
    - `OptimizedWallClimbersGameView` uses `CameraPreviewView()`; `WallClimbersScene.setupCameraPreview(in:)` is never invoked.
  - Net: multiple independent sessions (service + generic preview), and scenes‚Äô Vision pipelines aren‚Äôt actually running. This can cause device-in-use conflicts and wasted resources.

- __Vision orientation not specified__
  - `VNImageRequestHandler` is created without `orientation:` in `OptimizedMotionService` and scenes. For the front camera, Vision expects a correct `CGImagePropertyOrientation` (e.g., portrait front => `.leftMirrored`) to avoid flipped/rotated keypoints.

- __AVCapture configuration consistency__
  - Presets vary: service uses `.vga640x480` (good for Vision), `CameraPreviewView` uses `.medium`. Scenes don‚Äôt set `alwaysDiscardsLateVideoFrames`.
  - No explicit `videoOutput.videoSettings` pixel format (recommend 420f/YUV or BGRA).

- __Core Motion delivery on main__
  - `CMMotionManager.startDeviceMotionUpdates(using:to: OperationQueue.main)` at 100 Hz with an `@MainActor` service. This can saturate the main thread. Apple suggests delivering to a background queue and only hopping to main for UI mutations.
  - Reference frame `.xMagneticNorthZVertical` is sensitive to magnetic interference; `.xArbitraryCorrectedZVertical` is generally more stable indoors.

- __Timestamps__
  - Pose rep detection timestamps use `CACurrentMediaTime()`. Prefer `CMSampleBufferGetPresentationTimeStamp` for video-tied events.

- __Plist__
  - `NSCameraUsageDescription` and `NSMotionUsageDescription` present in [Config/Info.plist](cci:7://file:///Users/aadi/Desktop/FlexaSwiftUI/Config/Info.plist:0:0-0:0). Good.

I saved the ‚ÄúVision/Camera session unification + orientation handling plan‚Äù in memory to guide upcoming changes.

# Validation vs Apple Best Practices

- __Single capture pipeline + correct orientation__
  - Use one `AVCaptureSession` feeding Vision on a dedicated queue; share the session with any preview layers. Pass the correct `CGImagePropertyOrientation` to `VNImageRequestHandler`.
  - Refs:
    - VNDetectHumanBodyPoseRequest: https://developer.apple.com/documentation/vision/vndetecthumanbodyposerequest
    - VNImageRequestHandler: https://developer.apple.com/documentation/vision/vnimagerequesthandler
    - WWDC20 ‚ÄúDetect Body and Hand Pose with Vision‚Äù: https://developer.apple.com/videos/play/wwdc2020/10653/
    - Hand pose: https://developer.apple.com/documentation/vision/vndetecthumanhandposerequest

- __AVCapture performance__
  - Set `.vga640x480` for pose, `alwaysDiscardsLateVideoFrames = true`, and a fixed pixel format (typically 420f). Set `connection.videoOrientation = .portrait` and `connection.isVideoMirrored = true` for front camera to reduce extra transforms.

- __Core Motion__
  - Deliver to a background queue; sample at the lowest acceptable frequency (e.g., 50‚Äì60 Hz) and filter. Consider `.xArbitraryCorrectedZVertical` to reduce magnetic drift.
  - Refs:
    - CMMotionManager: https://developer.apple.com/documentation/coremotion/cmmotionmanager
    - Core Motion overview: https://developer.apple.com/documentation/coremotion

# Recommended Actions

- __Unify camera + Vision in `OptimizedMotionService`__
  - Make the service the sole owner of `AVCaptureSession` and Vision.
  - Expose the session for preview: pass `service.captureSession` into a revised `CameraPreviewView(session:)` so you preview the same session feeding Vision.
  - Remove per-scene capture sessions and Vision delegates. Scenes should consume `poseKeypoints`/derived metrics from the service (inject the service into scenes).

- __Fix Vision orientation__
  - Use `VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: <mapped>, options: [:])`.
  - Map from `AVCaptureConnection.videoOrientation` + front camera mirroring. For portrait front: `.leftMirrored` is typical.

- __Align configuration__
  - Force `.vga640x480`.
  - Set `videoOutput.alwaysDiscardsLateVideoFrames = true`.
  - Set `videoOutput.videoSettings` to `kCVPixelBufferPixelFormatTypeKey: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange`.
  - Set `connection.videoOrientation = .portrait` and `connection.isVideoMirrored = true` when using the front camera.

- __Core Motion improvements__
  - Start device motion updates on `motionQueue` instead of main.
  - Consider `deviceMotionUpdateInterval = 0.02` (50 Hz) with a small EMA/smoothing step; bump to 100 Hz only when necessary (active gameplay).
  - Use `.xArbitraryCorrectedZVertical`.

- __Scene hookups (quick fix vs. architecture)__
  - Quick fix: replace `CameraPreviewView()` with scene-backed preview (`ArmRaisesCameraPreviewView(scene:)`, `WallClimbersCameraPreviewView(scene:)`) so their delegates actually run. This still duplicates sessions.
  - Preferred: scenes stop owning camera; draw based on `OptimizedMotionService.poseKeypoints`.

- __Add instrumentation__
  - Count processed frames/sec, dropped frames, Vision latency. Log occasional stats to validate steady-state performance and energy impact.

# Minimal Code Sketches (illustrative)

- Use session from service in preview:
```swift
// CameraPreviewView: init with existing session
init(session: AVCaptureSession) { self.session = session }
```

- Vision orientation:
```swift
let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer,
                                    orientation: .leftMirrored, // portrait + front
                                    options: [:])
try? handler.perform([request])
```

- Core Motion off-main:
```swift
motionManager.startDeviceMotionUpdates(using: .xArbitraryCorrectedZVertical,
                                       to: motionQueue) { [weak self] motion, error in
  // compute off-main; hop to main only to publish
}
```

# Next Steps

- __Choose approach__: quick fix (scene-backed previews) vs. preferred architecture (single service session). I recommend the single-session approach to avoid conflicts and simplify.
- On approval, I‚Äôll implement:
  - Refactor `CameraPreviewView` to accept `AVCaptureSession`.
  - Wire `OptimizedMotionService.captureSession` into previews and scenes (remove scene-owned sessions).
  - Add orientation mapping and capture output settings.
  - Move Core Motion delivery to `motionQueue` and make sampling adaptive.

# References

- Apple Vision (Body/Hand Pose, Image Request Handler):  
  - https://developer.apple.com/documentation/vision/vndetecthumanbodyposerequest  
  - https://developer.apple.com/documentation/vision/vndetecthumanhandposerequest  
  - https://developer.apple.com/documentation/vision/vnimagerequesthandler  
  - WWDC20: https://developer.apple.com/videos/play/wwdc2020/10653/
- Core Motion Overview and CMMotionManager:  
  - https://developer.apple.com/documentation/coremotion  
  - https://developer.apple.com/documentation/coremotion/cmmotionmanager

# TODO Status

- t1 Inspect OptimizedMotionService and game views: completed
- t2 Search for Vision usage: completed
- t3 Research Vision best practices: in progress (findings above)
- t4 Research Core Motion best practices: in progress (findings above)
- t5 Validate against best practices; identify fixes: pending (awaiting approach decision)
- t6 Propose/implement code changes: pending (ready upon approval)

# Summary

Validated current architecture and identified key gaps: multiple camera sessions, missing Vision orientation, and motion delivery on main at 100 Hz. Proposed a single-session Vision pipeline via `OptimizedMotionService`, correct orientation handling, consistent capture settings, and Core Motion queue/sampling improvements. Ready to implement refactor upon your go-ahead. then build and we will see if everything works thanks!